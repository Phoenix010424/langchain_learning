{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# ZHUPU_API_KEY = '',

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "\n",
    "client = ZhipuAI(api_key='zhipuai_api_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf9738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completion(model='glm-4', created=1755741716, choices=[CompletionChoice(index=0, finish_reason='stop', message=CompletionMessage(content='äººå·¥æ™ºèƒ½åŠ©æ‰‹\\n\\nè¿™ä¸ªé—®é¢˜æ¶‰åŠå›½å®¶å’Œæ”¿æ²»é—®é¢˜ï¼Œéå¸¸å¤æ‚ã€‚é¦–å…ˆï¼Œä¸èƒ½ç®€å•åœ°è¯´â€œä»¥è‰²åˆ—å–œæ¬¢æˆ˜äº‰â€ï¼Œå› ä¸ºè¿™æ ·çš„æ¦‚æ‹¬æ˜¯ä¸å‡†ç¡®çš„ï¼Œä¹Ÿä¸å…¬å¹³ã€‚ä»¥è‰²åˆ—çš„å®‰å…¨å½¢åŠ¿ä¸€ç›´å¾ˆå¤æ‚ï¼Œå®ƒä¸å‘¨è¾¹å›½å®¶çš„å…³ç³»ç´§å¼ ï¼Œç‰¹åˆ«æ˜¯ä¸å·´å‹’æ–¯å¦çš„å†²çªå†å²æ‚ ä¹…ã€‚\\n\\nä»¥è‰²åˆ—è‡ª1948å¹´å»ºå›½ä»¥æ¥ï¼Œä¸€ç›´å¤„äºåœ°åŒºå†²çªçš„ä¸­å¿ƒã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å½±å“ä»¥è‰²åˆ—å®‰å…¨æ”¿ç­–å’Œå†›äº‹è¡ŒåŠ¨çš„å› ç´ ï¼š\\n\\n1. å®‰å…¨éœ€æ±‚ï¼šä»¥è‰²åˆ—åœ°ç†ä½ç½®ä½¿å…¶å¤„äºæˆ˜ç•¥ä¸Šçš„è„†å¼±çŠ¶æ€ï¼Œå›½åœŸé¢ç§¯å°ï¼Œäººå£å¯†é›†ï¼Œç¼ºä¹æˆ˜ç•¥æ·±åº¦ã€‚å› æ­¤ï¼Œä»¥è‰²åˆ—å¯¹å®‰å…¨å¨èƒéå¸¸æ•æ„Ÿï¼Œé€šå¸¸ä¼šé‡‡å–å…ˆå‘åˆ¶äººçš„ç­–ç•¥æ¥ä¿æŠ¤å…¶å›½æ°‘å’Œå›½å®¶ã€‚\\n\\n2. åœ°åŒºæ”¿æ²»ï¼šä¸­ä¸œåœ°åŒºçš„æ”¿æ²»åŠ¨æ€ï¼Œç‰¹åˆ«æ˜¯ä¸å·´å‹’æ–¯å¦ã€é»å·´å«©ã€å™åˆ©äºšå’Œä¼Šæœ—ç­‰å›½å®¶çš„å…³ç³»ï¼Œå¯¹ä»¥è‰²åˆ—çš„å®‰å…¨æ”¿ç­–å’Œå†›äº‹è¡ŒåŠ¨æœ‰å¾ˆå¤§å½±å“ã€‚\\n\\n3. å†å²å’Œæ–‡åŒ–ï¼šä»¥è‰²åˆ—çš„å†å²å’Œæ–‡åŒ–èƒŒæ™¯ï¼ŒåŒ…æ‹¬å¤§å± æ€çš„è®°å¿†å’Œå¯¹å›½å®¶å®‰å…¨çš„é‡è§†ï¼Œä¹Ÿæ˜¯å…¶å¯¹å¾…æˆ˜äº‰å’Œå†²çªçš„æ€åº¦çš„ä¸€éƒ¨åˆ†ã€‚\\n\\n4. å›½é™…æ”¯æŒï¼šä»¥è‰²åˆ—åœ¨å›½é™…èˆå°ä¸Šçš„è¡ŒåŠ¨å—åˆ°å…¶ç›Ÿå‹ï¼Œç‰¹åˆ«æ˜¯ç¾å›½çš„å½±å“å’Œæ”¯æŒã€‚\\n\\n5. å†…éƒ¨æ”¿æ²»ï¼šä»¥è‰²åˆ—å†…éƒ¨çš„æ”¿å…šæ”¿æ²»å’Œæ°‘æ„ä¹Ÿä¼šå½±å“å…¶æ”¿åºœå’Œå†›é˜Ÿçš„å†³ç­–ã€‚\\n\\né‡è¦çš„æ˜¯è¦è®¤è¯†åˆ°ï¼Œå¤§å¤šæ•°å›½å®¶å’Œäººæ°‘éƒ½ä¸å¸Œæœ›ç”Ÿæ´»åœ¨æˆ˜äº‰çŠ¶æ€ä¸­ï¼Œä»¥è‰²åˆ—äººæ°‘ä¹Ÿä¸ä¾‹å¤–ã€‚æˆ˜äº‰å¸¦æ¥çš„æ˜¯ç ´åã€ç—›è‹¦å’ŒæŸå¤±ï¼Œå¤§å¤šæ•°äººéƒ½å¸Œæœ›å’Œå¹³ä¸ç¨³å®šã€‚\\n\\nåœ¨è®¨è®ºè¿™æ ·çš„é—®é¢˜æ—¶ï¼Œåº”è¯¥ä¿æŒå®¢è§‚å’Œå¤šå…ƒçš„è§†è§’ï¼Œé¿å…ç®€åŒ–å’Œä¸€æ¦‚è€Œè®ºçš„è¡¨è¿°ã€‚', role='assistant', reasoning_content=None, tool_calls=None))], request_id='202508211001449f1340a7d24347fb', id='202508211001449f1340a7d24347fb', usage=CompletionUsage(prompt_tokens=17, completion_tokens=289, total_tokens=306))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "client = ZhipuAI(api_key='zhipuai_api_key')\n",
    "prompt = \"ä»¥è‰²åˆ—ä¸ºä»€ä¹ˆå–œæ¬¢æˆ˜äº‰ï¼Ÿ\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages=[\n",
    "        {\"role\":\"user\",\"content\":\"ä½ å¥½\"},\n",
    "        {\"role\":\"assistant\",\"content\":\"æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹\"},\n",
    "        {\"role\":\"user\",\"content\":prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "client = ZhipuAI(api_key='zhipuai_api_key')\n",
    "prompt = \"glm-4åŸç†æ˜¯ä»€ä¹ˆï¼Ÿä½¿ç”¨äº†å¤šå°‘çš„å‚æ•°è¿›è¡Œè®­ç»ƒï¼Ÿ\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages=[\n",
    "        {\"role\":\"user\",\"content\":\"ä½ å¥½\"},\n",
    "        {\"role\":\"assistant\",\"content\":\"æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹\"},\n",
    "        {\"role\":\"user\",\"content\":prompt}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6771fdd",
   "metadata": {},
   "source": [
    "#### langchainæ¡†æ¶çš„æµå¼è¾“å‡ºä¸ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee12192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"è¯·æ ¹æ®ä¸‹é¢çš„ä¸»é¢˜å†™ä¸€ç¯‡å°çº¢ä¹¦è¥é”€çš„çŸ­æ–‡ï¼š {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\",\n",
    "                   openai_api_key='OPENAI_API_KEY',\n",
    "                   streaming=True)  # æ˜¾å¼å¯ç”¨æµå¼ï¼ˆé»˜è®¤Trueï¼Œä½†å»ºè®®æ˜ç¡®å£°æ˜ï¼‰\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"topic\": \"åº·å¸ˆå‚…ç»¿èŒ¶\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.llms.base import LLM\n",
    "from zhipuai import ZhipuAI\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "\n",
    "class ChatGLM4(LLM):\n",
    "    history = []\n",
    "    client:object = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        zhipuai_api_key = os.getenv('ZHUPU_API_KEY')\n",
    "        self.client = ZhipuAI(api_key=zhipuai_api_key)\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"ChatGLM4\"\n",
    "    \n",
    "    def invoke(self,prompt,config={},history=[]):\n",
    "        if history is None:\n",
    "            history=[]\n",
    "        if not isinstance(prompt, str):\n",
    "            prompt = prompt.to_string()\n",
    "        history.append({\"role\":\"user\",\"content\":prompt })\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"glm-4\",\n",
    "            messages=history\n",
    "        )\n",
    "\n",
    "        result = response.choices[0].message.content\n",
    "        return AIMessage(content=result)\n",
    "        \n",
    "    def _call(self,prompt,config,history=[]):\n",
    "        return self.invoke(prompt,history)\n",
    "        \n",
    "    def stream(self,prompt,config,history=[]):\n",
    "        if history is None:\n",
    "            history=[]\n",
    "        if not isinstance(prompt, str):\n",
    "            prompt = prompt.to_string()\n",
    "        history.append({\"role\":\"user\",\"content\":prompt})\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"glm-4\",\n",
    "            messages=history,\n",
    "            stream=True\n",
    "        )\n",
    "        for chunk in response:\n",
    "            yield chunk.choices[0].delta.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37991efb",
   "metadata": {},
   "source": [
    "### æ•™æˆè®²è§£ï¼šLangChainè‡ªå®šä¹‰LLMé›†æˆChatGLM4\n",
    "\n",
    "### ä¸€ã€æ ¸å¿ƒç»„ä»¶è§£æï¼ˆè§£å‰–AIçš„\"ç¥ç»ç³»ç»Ÿ\"ï¼‰\n",
    "1. **åŸºå› ç»§æ‰¿**  \n",
    "   `class ChatGLM4(LLM)`ï¼š  \n",
    "   è¿™æ˜¯LangChainæ¡†æ¶çš„**æ ‡å‡†ç»§æ‰¿èŒƒå¼**ï¼Œå°±åƒäººç±»ç»§æ‰¿DNAã€‚é€šè¿‡ç»§æ‰¿`LLM`åŸºç±»ï¼Œè·å¾—äº†LangChainç”Ÿæ€çš„å¤©ç„¶å…¼å®¹æ€§ï¼Œè®©è¿™ä¸ªè‡ªå®šä¹‰æ¨¡å‹èƒ½æ— ç¼æ¥å…¥LangChainçš„å·¥ä½œæµã€‚\n",
    "\n",
    "2. **ä¸‰å¤§å…³é”®æ¨¡å—**  \n",
    "   ```python\n",
    "   from langchain.llms.base import LLM          # åŸºç±»\n",
    "   from zhipuai import ZhipuAI                  # å›½äº§å¤§æ¨¡å‹æ¥å£\n",
    "   from langchain_core.messages.ai import AIMessage  # æ¶ˆæ¯å°è£…\n",
    "   ```\n",
    "   - `ZhipuAI`æ˜¯è¿æ¥æ™ºè°±AIçš„\"ç¥ç»å¯¼ç®¡\"\n",
    "   - `AIMessage`æ˜¯LangChainçš„\"æ ‡å‡†è¯­è¨€åŒ…\"ï¼Œç¡®ä¿è¾“å‡ºèƒ½è¢«å…¶ä»–ç»„ä»¶ç†è§£\n",
    "\n",
    "---\n",
    "\n",
    "### äºŒã€åˆå§‹åŒ–è¿‡ç¨‹ï¼ˆç»™AI\"ä¸Šç”µå¯åŠ¨\"ï¼‰\n",
    "```python\n",
    "def __init__(self):\n",
    "    super().__init__()\n",
    "    zhipuai_api_key = os.getenv('ZHUPU_API_KEY')  # å®‰å…¨å¯†é’¥è¯»å–\n",
    "    self.client = ZhipuAI(api_key=zhipuai_api_key)  # åˆ›å»ºAPIè¿æ¥\n",
    "```\n",
    "- **ç¯å¢ƒå˜é‡ä¿æŠ¤**ï¼šé€šè¿‡`os.getenv`è·å–APIå¯†é’¥ï¼Œé¿å…ç¡¬ç¼–ç æ³„éœ²\n",
    "- **å®¢æˆ·ç«¯å®ä¾‹åŒ–**ï¼šåˆ›å»ºæ°¸ä¹…çš„é€šä¿¡ç®¡é“ï¼Œåç»­è°ƒç”¨æ— éœ€é‡å¤å»ºç«‹è¿æ¥\n",
    "\n",
    "> ğŸ” å®‰å…¨æç¤ºï¼šå°±åƒä¿ç®¡é“¶è¡Œå¡å¯†ç ï¼Œæ°¸è¿œä¸è¦å°†APIå¯†é’¥ç›´æ¥å†™åœ¨ä»£ç é‡Œï¼\n",
    "\n",
    "---\n",
    "\n",
    "### ä¸‰ã€æ ¸å¿ƒèƒ½åŠ›å®ç°ï¼ˆAIçš„\"æ€è€ƒè¿‡ç¨‹\"ï¼‰\n",
    "\n",
    "#### 1. **èº«ä»½å£°æ˜**  \n",
    "   ```python\n",
    "   @property\n",
    "   def _llm_type(self):\n",
    "       return \"ChatGLM4\"\n",
    "   ```\n",
    "   - è¿™æ˜¯LangChainçš„**å¼ºåˆ¶è¦æ±‚**ï¼Œç›¸å½“äºAIçš„\"èº«ä»½è¯\"\n",
    "   - æ¡†æ¶é€šè¿‡æ­¤å±æ€§è¯†åˆ«æ¨¡å‹ç±»å‹ï¼Œç”¨äºæ—¥å¿—/ç›‘æ§ç­‰ç³»ç»Ÿ\n",
    "\n",
    "#### 2. **åŒæ­¥è°ƒç”¨** - `invoke()`æ–¹æ³•\n",
    "   ```python\n",
    "   history.append({\"role\":\"user\",\"content\":prompt })  # æ„å»ºå¯¹è¯è®°å¿†\n",
    "   response = self.client.chat.completions.create(\n",
    "        model=\"glm-4\",  # æŒ‡å®šæ¨¡å‹ç‰ˆæœ¬\n",
    "        messages=history  # ä¼ å…¥å®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡\n",
    "   )\n",
    "   return AIMessage(content=result)  # æ ‡å‡†åŒ–å°è£…\n",
    "   ```\n",
    "   - **å¯¹è¯è¿ç»­æ€§**ï¼šé€šè¿‡`history`å®ç°å¤šè½®å¯¹è¯è®°å¿†\n",
    "   - **æ™ºèƒ½è·¯ç”±**ï¼šè‡ªåŠ¨å°†éå­—ç¬¦ä¸²promptè½¬ä¸ºæ–‡æœ¬ï¼ˆ`prompt.to_string()`ï¼‰\n",
    "   - **è¾“å‡ºæ ‡å‡†åŒ–**ï¼šè¿”å›`AIMessage`å¯¹è±¡è€ŒéåŸå§‹æ–‡æœ¬ï¼Œç¡®ä¿ä¸å…¶ä»–LangChainç»„ä»¶å…¼å®¹\n",
    "\n",
    "#### 3. **æµå¼è¾“å‡º** - `stream()`æ–¹æ³•\n",
    "   ```python\n",
    "   response = self.client.chat.completions.create(\n",
    "        stream=True,  # å¼€å¯æµæ¨¡å¼\n",
    "        ... \n",
    "   )\n",
    "   for chunk in response:\n",
    "        yield chunk.choices[0].delta.content  # å®æ—¶ç”Ÿæˆ\n",
    "   ```\n",
    "   - **é€å­—ç”Ÿæˆ**ï¼šåƒæ°´æµä¸€æ ·å®æ—¶è¿”å›ç»“æœï¼Œæå‡ç”¨æˆ·ä½“éªŒ\n",
    "   - **å†…å­˜ä¼˜åŒ–**ï¼šé€‚åˆé•¿æ–‡æœ¬ç”Ÿæˆï¼Œé¿å…å†…å­˜æš´æ¶¨\n",
    "   - **æŠ€æœ¯äº®ç‚¹**ï¼šä½¿ç”¨`yield`å®ç°ç”Ÿæˆå™¨ï¼Œæ˜¯Pythonçš„é«˜çº§ç‰¹æ€§\n",
    "\n",
    "---\n",
    "\n",
    "### å››ã€è®¾è®¡æ¨¡å¼ç²¾è¦ï¼ˆæ¶æ„å¸ˆçš„æ€ç»´ï¼‰\n",
    "1. **é€‚é…å™¨æ¨¡å¼**  \n",
    "   åœ¨`_call()`æ–¹æ³•ä¸­æ¡¥æ¥LangChainæ ‡å‡†æ¥å£å’Œå®é™…å®ç°ï¼š\n",
    "   ```python\n",
    "   def _call(self, prompt, config, history=[]):\n",
    "        return self.invoke(prompt, history)  # è·¯ç”±åˆ°å…·ä½“å®ç°\n",
    "   ```\n",
    "\n",
    "2. **é˜²å¾¡å¼ç¼–ç¨‹**  \n",
    "   ```python\n",
    "   if history is None: history=[]  # é˜²Noneä¿æŠ¤\n",
    "   if not isinstance(prompt, str): ... # ç±»å‹å®‰å…¨\n",
    "   ```\n",
    "\n",
    "3. **ç¯å¢ƒéš”ç¦»**  \n",
    "   é€šè¿‡`config`å‚æ•°é¢„ç•™æ‰©å±•èƒ½åŠ›ï¼Œæœªæ¥å¯æ”¯æŒï¼š\n",
    "   - æ¸©åº¦è°ƒèŠ‚\n",
    "   - æœ€å¤§ç”Ÿæˆé•¿åº¦\n",
    "   - æƒ©ç½šç³»æ•°ç­‰é«˜çº§å‚æ•°\n",
    "\n",
    "---\n",
    "\n",
    "### äº”ã€å®æˆ˜æ”¹è¿›å»ºè®®ï¼ˆå·¥ç¨‹å¸ˆçš„è°ƒä¼˜ç¬”è®°ï¼‰\n",
    "1. **å†å²ç®¡ç†ä¼˜åŒ–**  \n",
    "   å½“å‰ç›´æ¥ä¿®æ”¹å¤–éƒ¨ä¼ å…¥çš„`history`åˆ—è¡¨å­˜åœ¨é£é™©ï¼Œå»ºè®®æ”¹ä¸ºï¼š\n",
    "   ```python\n",
    "   new_history = history.copy()  # åˆ›å»ºå‰¯æœ¬\n",
    "   new_history.append({\"role\":\"user\",\"content\":prompt})\n",
    "   ```\n",
    "\n",
    "2. **é”™è¯¯å¤„ç†å¢å¼º**  \n",
    "   å¢åŠ APIè°ƒç”¨å¼‚å¸¸å¤„ç†ï¼š\n",
    "   ```python\n",
    "   try:\n",
    "        response = self.client.chat.completions.create(...)\n",
    "   except APIError as e:\n",
    "        logger.error(f\"APIè°ƒç”¨å¤±è´¥: {e}\")\n",
    "        return AIMessage(content=\"æœåŠ¡æš‚æ—¶ä¸å¯ç”¨\")\n",
    "   ```\n",
    "\n",
    "3. **é…ç½®å‚æ•°é€ä¼ **  \n",
    "   æ”¹è¿›`_call`æ–¹æ³•æ”¯æŒåŠ¨æ€å‚æ•°ï¼š\n",
    "   ```python\n",
    "   def _call(self, prompt, stop=None, **kwargs):\n",
    "        config = kwargs.get('config', {})\n",
    "        return self.invoke(prompt, config)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### å…­ã€çŸ¥è¯†å»¶ä¼¸\n",
    "1. **LangChainæ¶æ„å›¾**  \n",
    "   ```mermaid\n",
    "   graph LR\n",
    "   A[ä½ çš„åº”ç”¨] --> B[LangChain Chain]\n",
    "   B --> C[è‡ªå®šä¹‰ChatGLM4]\n",
    "   C --> D[æ™ºè°±AIäº‘æœåŠ¡]\n",
    "   ```\n",
    "\n",
    "2. **é€‚ç”¨åœºæ™¯**  \n",
    "   - ä¼ä¸šçº§æ™ºèƒ½å®¢æœç³»ç»Ÿ\n",
    "   - æœ¬åœ°çŸ¥è¯†åº“é—®ç­”\n",
    "   - ç»“åˆLangChainçš„Agentå®ç°è‡ªåŠ¨åŒ–ä»»åŠ¡\n",
    "\n",
    "**æ€»ç»“**ï¼šè¿™ä¸ªå®ç°å±•ç°äº†LangChainæ¡†æ¶çš„å¼ºå¤§æ‰©å±•èƒ½åŠ›ï¼Œå°±åƒç»™å›½äº§å¤§æ¨¡å‹è£…ä¸Šäº†é€šç”¨é€‚é…å™¨ã€‚é€šè¿‡æ ‡å‡†åŒ–æ¥å£è®¾è®¡ï¼Œä½¿ChatGLM4èƒ½èå…¥å…¨çƒAIç”Ÿæ€ï¼Œè¿™æ­£æ˜¯LangChainçš„ç²¾é«“æ‰€åœ¨ï¼\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
